{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction of a public dataset of Blood Bowl matches played on FUMBBL.com\n",
    "\n",
    "This notebook uses Python API and HTML scraping to fetch online Blood Bowl match outcome data, and to create a structured dataset ready for analysis and visualization. \n",
    "\n",
    "The idea is to make Blood Bowl data analysis accessible to others. \n",
    "Using open source tooling reduces the barriers for others to build on other peopleâ€™s work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import os\n",
    "\n",
    "from isoweek import Week\n",
    "\n",
    "import requests # API library\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pm add selenium for CSR in BBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from selenium import webdriver\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# driver = webdriver.Chrome(ChromeDriverManager().install()) # 70-114\n",
    "\n",
    "# # Navigate to the webpage\n",
    "# url = 'https://example.com'\n",
    "# driver.get(url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blood Bowl online: FUMBBL \n",
    "\n",
    "The **FUMBBL** website (https://fumbbl.com) is one big pile of data. From coach pages, with their teams, to team rosters, with players, and match histories. It's all there.\n",
    "\n",
    "To obtain **FUMBBL** data, we need to fetch it match by match, team by team. To do so, the site creator Christer Kaivo-oja, from Sweden, has made an API that allows us to easily fetch data. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Python requests package to fetch data\n",
    "We use the [Python **Requests** library](https://docs.python-requests.org/en/latest/) to make the API call over HTTPS and obtain the response from the FUMBLL server. The response is in the JSON format, a [light-weight data-interchange format](https://www.json.org/json-en.html) which is both easy to read and write for humans, and easy to parse and generate by computers. So this makes it a natural choice for an API.\n",
    "The full documentation of the API can be found at (https://fumbbl.com/apidoc/).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data structure\n",
    "\n",
    "We go with a flat data frame with **rows for each match**, and columns for the various variables associated with each match.\n",
    "These would include:\n",
    "\n",
    "* Coach ids\n",
    "* Team races\n",
    "* Team ids\n",
    "* Date of the match\n",
    "* Outcome (Touchdowns of both teams)\n",
    "\n",
    "With this basic structure, we can add as many match related variables in the future, keeping the basic structure (each row is a match) unchanged.\n",
    "\n",
    "So lets get the match data!\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: API scraping the match data: df_matches\n",
    "\n",
    "So we are mostly interested in the current ruleset, this is `BB2020`. This ruleset became available in **FUMBBL** at september 1st 2021, and two months later, some 5000 games have been played. We also want to compare with the previous ruleset, where we have much more data available. \n",
    "The dataset start with match `4216258` played on august 1st, 2020. This covers roughly 12 months of `BB2016` ruleset matches, after that it switches to predominantly `BB2020` matches.\n",
    "\n",
    "We collect match data by looping over `match_id`. We store the full JSON file on disk, so we avoid repeat API calls in the future.\n",
    "\n",
    "**VERY IMPORTANT: We do not want to overload the **FUMBBL** server, so we make only three API requests per second. In this way, the server load is hardly affected and it can continue functioning properly for all the Blood Bowl coaches playing their daily games!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/transform_match_data.py\n",
    "%run src/write_json_file.py\n",
    "%run src/update_match_jsons.py\n",
    "%run src/process_match_jsons.py\n",
    "%run src/scrape_match_htmls.py\n",
    "%run src/process_html_files.py\n",
    "%run src/update_tourney_jsons.py\n",
    "%run src/process_tourney_jsons.py\n",
    "%run src/update_group_jsons.py\n",
    "%run src/process_group_jsons.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SET SCRAPE PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "COMPLETE_UP_TO = 4575398\n",
    "UPDATE_TO = 4591579\n",
    "SCRAPE_TIMESTAMP = '_20241209.h5'\n",
    "\n",
    "################################################################################\n",
    "# move end match to start fetch math\n",
    "BEGIN_MATCH = 4216259 # august 2020\n",
    "\n",
    "BEGIN_JSON_MATCH = COMPLETE_UP_TO # BEGIN_JSON_FETCH\n",
    "BEGIN_SCRAPE_MATCH = COMPLETE_UP_TO # BEGIN_HTML_FETCH\n",
    "\n",
    "BEGIN_HTML_MATCH = COMPLETE_UP_TO # BEGIN_HTML_PROCESS\n",
    "END_HTML_MATCH = UPDATE_TO # END_HTML_PROCESS\n",
    "\n",
    "END_MATCH = UPDATE_TO  \n",
    "END_SCRAPE_MATCH = UPDATE_TO\n",
    "\n",
    "FETCH_NEW_JSONS = 1\n",
    "PROCESS_JSONS = 1 # 2min for 350k matches\n",
    "DF_MATCHES_RAW = 'raw/df_matches' + SCRAPE_TIMESTAMP\n",
    "SCRAPE_NEW_HTMLS = 1\n",
    "PROCESS_HTMLS = 1 # MOST TIME CONSUMING STEP\n",
    "\n",
    "FETCH_NEW_TOURNEYS = 1\n",
    "PROCESS_TOURNEYS = 1\n",
    "DF_TOURNEYS_RAW = 'raw/df_tourneys' + SCRAPE_TIMESTAMP\n",
    "FETCH_NEW_GROUPS = 1\n",
    "PROCESS_GROUPS = 1\n",
    "DF_GROUPS_RAW = 'raw/df_groups' + SCRAPE_TIMESTAMP\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_match_jsons(full_run = FETCH_NEW_JSONS, begin_match = BEGIN_JSON_MATCH, end_match = END_MATCH, verbose=True) # takes 17s if no new matches\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the match JSONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we need as begin_match the very first match on disk\n",
    "df_matches = process_match_jsons(full_run = PROCESS_JSONS, begin_match = BEGIN_MATCH, end_match = END_MATCH, verbose= True, target_file = DF_MATCHES_RAW)\n",
    "\n",
    "df_matches.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataprep: transforming the data\n",
    "\n",
    "Since we manually filled the `pandas` DataFrame, most of the columns are now of `object` datatype.\n",
    "We need to change this to be able to work properly with the data, as well as store it properly.\n",
    "Here I convert each column manually, however I later found out about `DataFrame.infer_objects()`, that can detect the proper dtype automatically.\n",
    "This I will try next time.\n",
    "\n",
    "In Blood Bowl, teams can develop themselves over the course of multiple matches. The winnings of each match can be spend on buying new, stronger players, or replace the players that ended up getting injured or even killed. In addition, players receive so-called *star points* for important events, such as scoring, or inflicting a casualty on the opponent. Therefore, a balancing mechanism is needed when a newly created \"rookie\" team is facing a highly developed opposing team with lots of extra skills and strong players. \n",
    "\n",
    "Blood Bowl solves this by calculating for both teams their **Current team value**.\n",
    "The **Team value difference** for a match determines the amount of gold that the weaker team can use to buy so-called **inducements**.\n",
    "These inducements are temporary, and can consists of a famous \"star player\" who joins the team just for this match. Another popular option is to hire a wizard that can be used to turn one of the opposing players into a frog.\n",
    "\n",
    "It is well known that the win rates of the teams depend on how developed a team is. For example, Amazons are thought to be strongest at low team value, as they already start out with lots of *block* and *dodge* skills, whereas a Chaos team start out with almost no skills.\n",
    "So if we compare win rates, we would like take into account the current team value. \n",
    "Now as this can differ between the two teams in a match up, I reasoned that the highest team value is most informative about the average strength level of both teams, because of the inducement mechanism described above. (In the next step, we will add information on inducements)\n",
    "\n",
    "In the dataset, we have for each match the current team values of both teams as a text string. \n",
    "We transform the text string `1100k` into an integer number `1100`, so that we can calculated the difference as `tv_diff`, and pick for each match the maximum team value and store it as `tv_match`. Finally, we create a team value bin `tv_bin` to be able to compare win rates for binned groups of matches where races have comparable team strength / team development.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix API changes\n",
    "\n",
    "\n",
    "Newly fetched matches (from match id 4474451 onwards)\n",
    "\n",
    "At some point teamValue as string with a k was changed to teamValue in gold and currentTeamValue in gold. Fix this in the data.\n",
    "\n",
    "NOTE: WE ARE USING TV BEFORE INDUCEMENTS. IF THE TEAM WITH THE HIGHER TV DECIDES TO INDUCE WE DO NOT CAPTURE THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches.iloc[258151] # just divide by 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches = transform_match_data(df_matches)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches.iloc[258151]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping empty matches\n",
    "\n",
    "Some match_id's do not have match information attached to them, presumably these matches were not played or some real life event interfered. These match_ids are dropped from the dataset to get rid of the NAs in all the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_matches)\n",
    "df_matches = df_matches.dropna(subset=['match_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataprep: getting the dates right\n",
    "\n",
    "To see time trends, its useful to aggregate the data by week. For this we add `week_number` for each date, and from this week number, we convert back to a date to get a `week_date`. This last part is useful for plotting with `plotnine`, as this treats dates in a special way\n",
    "We use the ISO definition of week, this has some unexpected behavior near the beginning / end of each year. \n",
    "\n",
    "The data starts in week 36 (september) of 2020, and stops halfway march 2022.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches['week_number'] = df_matches['match_date'].dt.isocalendar().week\n",
    "\n",
    "# cannot serialize numpy int OR Int64 when writing HDF5 file, so we go for plain int as all NAs are gone now\n",
    "df_matches['week_number'] = df_matches['week_number'].fillna(0).astype(int)\n",
    "\n",
    "# add year based on match ISO week\n",
    "df_matches['year'] = df_matches['match_date'].dt.isocalendar().year.astype(int)\n",
    "\n",
    "df_matches['week_year'] = df_matches['year'].astype(str) + '-' + df_matches['week_number'].astype(str)\n",
    "\n",
    "# use a lambda function since isoweek.Week is not vectorized \n",
    "df_matches['week_date'] = pd.to_datetime(df_matches.apply(lambda row : Week(int(row[\"year\"]),int(row[\"week_number\"])).monday(),axis=1))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: HTML Scraping more match related data\n",
    "\n",
    "Next, we collect more match related data to add to  `df_matches`.\n",
    "For example, the **inducements** and **coach rankings**, as well as match performance stats such as total passing distance `pass`. \n",
    "This information is not available through the API, but each played match has an associated HTML page at https://fumbbl.com/FUMBBL.php?page=match with more info.\n",
    "\n",
    "Since we have to fetch the complete HTML page for each match anyways, I decided to split the proces in two steps:\n",
    "\n",
    "In the first step, the HTML pages for the desired matches are fetched and stored on disk. \n",
    "After some optimization fetching 1K matches takes 10 min. So 6K matches per hour.\n",
    "\n",
    "(Total file size for 154K files is 7GB. These files cannot be stored on Github.)\n",
    "\n",
    "In the second step, the HTML pages are processed with `BeautifulSoup` to extract inducments and coach rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_match_htmls(full_run = SCRAPE_NEW_HTMLS, begin_match = BEGIN_SCRAPE_MATCH, end_match = END_SCRAPE_MATCH, verbose= True)   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the match HTML files\n",
    "\n",
    "I highly recommend [this tutorial](https://hackersandslackers.com/scraping-urls-with-beautifulsoup/) for a great introduction to `BeautifulSoup`.\n",
    "It allows easy access to the information contained within HTML \"div\" tags that partition the web page in different sections. \n",
    "\n",
    "In addition, to clean up the scraped text, I used the **re** Python module (Regular expressions), part of the [Python standard library](https://docs.python.org/3/library/index.html) to extract the actual inducements from the text string that contains them.\n",
    "\n",
    "Now it takes 1.5 min for 1K matches. 40K matches, expect 1h. check\n",
    "Now it takes 3h for 90K matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PM rewrite to produce a JSON per match with key value pairs\n",
    "\n",
    "# read from hdf5 file    \n",
    "df_matches_html1 = pd.read_hdf('raw/df_matches_html_20220315_220825.h5') # old CR\n",
    "df_matches_html2 = pd.read_hdf('raw/df_matches_html_20220316_133752.h5') # old CR \n",
    "df_matches_html3 = pd.read_hdf('raw/df_matches_html_20220608_054453.h5') # old CR\n",
    "df_matches_html4 = pd.read_hdf('raw/df_matches_html_20230115_133734.h5') # new CR\n",
    "df_matches_html5 = pd.read_hdf('raw/df_matches_html_20230526_080359.h5')\n",
    "df_matches_html6 = pd.read_hdf('raw/df_matches_html_20230728_185951.h5')\n",
    "df_matches_html7 = pd.read_hdf('raw/df_matches_html_20240731_231611.h5')\n",
    "df_matches_html8 = pd.read_hdf('raw/df_matches_html_20240916_193801.h5')\n",
    "df_matches_html = pd.concat([df_matches_html1, df_matches_html2, \n",
    "                                df_matches_html3, df_matches_html4, \n",
    "                                df_matches_html5, df_matches_html6, df_matches_html7\n",
    "                                , df_matches_html8], ignore_index= True)\n",
    "\n",
    "if PROCESS_HTMLS:\n",
    "    df_matches_html_ = process_html_files(begin_match = BEGIN_HTML_MATCH, end_match = END_HTML_MATCH, verbose= True)\n",
    "    # write data as hdf5 file\n",
    "    df_matches_html_.to_hdf('raw/df_matches_html_20240916_193801.h5', key='df_matches_html', mode='w')\n",
    "    df_matches_html = pd.concat([df_matches_html, df_matches_html_], ignore_index= True)\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix the performance stats\n",
    "\n",
    "The completions have a few weird values like `4/1`, we drop the slash and the value behind that.\n",
    "`-` is converted to 0 when the string ends directly after the `-` character, i.e. `-` becomes 0 but `-1` becomes -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches_html['team1_comp'] = df_matches_html['team1_comp'].str.replace(r'(/).*','')\n",
    "df_matches_html['team1_comp'] = pd.to_numeric(df_matches_html['team1_comp'].str.replace(r'-$','0'))\n",
    "\n",
    "df_matches_html['team2_comp'] = df_matches_html['team2_comp'].str.replace(r'(/).*','')\n",
    "df_matches_html['team2_comp'] = pd.to_numeric(df_matches_html['team2_comp'].str.replace(r'-$','0'))\n",
    "\n",
    "df_matches_html['team1_pass'] = pd.to_numeric(df_matches_html['team1_pass'].str.replace(r'-$','0'))\n",
    "df_matches_html['team2_pass'] = pd.to_numeric(df_matches_html['team2_pass'].str.replace(r'-$','0'))\n",
    "\n",
    "df_matches_html['team1_rush'] = pd.to_numeric(df_matches_html['team1_rush'].str.replace(r'-$','0'))\n",
    "df_matches_html['team2_rush'] = pd.to_numeric(df_matches_html['team2_rush'].str.replace(r'-$','0'))\n",
    "\n",
    "df_matches_html['team1_block'] = pd.to_numeric(df_matches_html['team1_block'].str.replace(r'-$','0'))\n",
    "df_matches_html['team2_block'] = pd.to_numeric(df_matches_html['team2_block'].str.replace(r'-$','0'))\n",
    "\n",
    "df_matches_html['team1_foul'] = pd.to_numeric(df_matches_html['team1_foul'].str.replace(r'-$','0'))\n",
    "df_matches_html['team2_foul'] = pd.to_numeric(df_matches_html['team2_foul'].str.replace(r'-$','0'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches_html.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches_html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataprep: coach rankings\n",
    "\n",
    "We want to extract the part `CR 149.99` from the scraped coach information field (example `test_string` below). Just as we matches on `Inducements:`, we can match on `CR ` and grab the contents directly after that, stopping when we encounter a whitespace.\n",
    "\n",
    "We first play around a bit and test until we discover the proper Regular Expression to use :-)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = 'kingcann Emerging Star CR 149.99 (-0.76)'\n",
    "\n",
    "pattern = re.compile(r'.*CR (.*)\\s\\(.*')\n",
    "\n",
    "match = re.match(pattern, test_string)\n",
    "\n",
    "if match is not None:\n",
    "    print(match.group(1)) # group(0) is the whole string\n",
    "else:\n",
    "    print(\"match is none\")\n",
    "\n",
    "test_string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Got 'm! Now that we have figured it out, we can write the code that extracts the coach rankings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataprep fix match_id\n",
    "df_matches_html['match_id'] = pd.to_numeric(df_matches_html.match_id) \n",
    "\n",
    "# Dataprep: add the coach rankings as separate cols\n",
    "df_matches_html['coach1_CR'] = df_matches_html['coach1_ranking'].str.extract(r'.*CR (.*)\\s\\(.*')\n",
    "df_matches_html['coach2_CR'] = df_matches_html['coach2_ranking'].str.extract(r'.*CR (.*)\\s\\(.*')\n",
    "\n",
    "df_matches_html['coach1_CR'] = pd.to_numeric(df_matches_html['coach1_CR'])\n",
    "df_matches_html['coach2_CR'] = pd.to_numeric(df_matches_html['coach2_CR'])\n",
    "\n",
    "# if > 400 divide by 10\n",
    "df_matches_html.loc[df_matches_html['coach1_CR'] > 400, 'coach1_CR'] = df_matches_html.loc[df_matches_html['coach1_CR'] > 400, 'coach1_CR']/10\n",
    "df_matches_html.loc[df_matches_html['coach2_CR'] > 400, 'coach2_CR'] = df_matches_html.loc[df_matches_html['coach2_CR'] > 400, 'coach2_CR']/10\n",
    "# USE NP.WHERE\n",
    "# abs\n",
    "df_matches_html['CR_diff'] = np.abs(df_matches_html['coach1_CR'] - df_matches_html['coach2_CR'])\n",
    "df_matches_html['CR_diff'] = df_matches_html['CR_diff'].astype(float)\n",
    "\n",
    "# +/-\n",
    "df_matches_html['cr_diff2'] = df_matches_html['coach1_CR'] - df_matches_html['coach2_CR']\n",
    "\n",
    "df_matches_html['cr_diff2_bin'] = pd.cut(df_matches_html['cr_diff2'], bins = [-1*float(\"inf\"), -30, -20, -10, -5, 5, 10, 20, 30, float(\"inf\")], \n",
    " labels=['{-Inf,-30]', '[-30,-20]', '[-20,-10]', '[-10,-5]', '[-5,5]', '[5,10]', '[10,20]', '[20,30]', '[30,Inf]']) \n",
    "\n",
    "df_matches_html['coach1_CR_bin'] = pd.cut(df_matches_html['coach1_CR'], \n",
    "    bins = [0, 135,145, 155, 165, 175, float(\"inf\")], \n",
    "    labels=['CR135-', 'CR140', 'CR150', 'CR160','CR170', 'CR175+'])\n",
    "\n",
    "df_matches_html['coach2_CR_bin'] = pd.cut(df_matches_html['coach2_CR'], \n",
    "    bins = [0, 135,145, 155, 165, 175, float(\"inf\")], \n",
    "    labels=['CR135-', 'CR140', 'CR150', 'CR160','CR170', 'CR175+'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches_html['coach2_CR_bin'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataprep match inducements for each team\n",
    "\n",
    "The next trick is to use `pandas` `explode()` method (similar to `separate_rows()` in `tidyverse` R) to give each inducement its own row in the dataset.\n",
    "This creates a dataframe (`inducements`) similar to `df_mbt` with each match generating at least two rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team1_inducements = df_matches_html[['match_id', 'team1_inducements']]\n",
    "team2_inducements = df_matches_html[['match_id', 'team2_inducements']]\n",
    "\n",
    "# make column names equal\n",
    "team1_inducements.columns = team2_inducements.columns = ['match_id', 'inducements']\n",
    "team1_inducements['team'] = 'team1'\n",
    "team2_inducements['team'] = 'team2'\n",
    "\n",
    "# row bind the two dataframes\n",
    "inducements = pd.concat([team1_inducements, team2_inducements], ignore_index = True)\n",
    "\n",
    "# convert comma separated string to list\n",
    "inducements['inducements'] = inducements['inducements'].str.split(',')\n",
    "\n",
    "# make each element of the list a separate row\n",
    "inducements = inducements.explode('inducements')\n",
    "\n",
    "# strip leading and trailing whitespaces\n",
    "inducements['inducements'] = inducements['inducements'].str.strip()\n",
    "\n",
    "# create \"star player\" label\n",
    "inducements['star_player'] = 0\n",
    "inducements.loc[inducements['inducements'].str.contains(\"Star player\"), 'star_player'] = 1\n",
    "\n",
    "# create \"card\" label\n",
    "inducements['special_card'] = 0\n",
    "inducements.loc[inducements['inducements'].str.contains(\"Card\"), 'special_card'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inducements"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add match HTML data to df_matches\n",
    "\n",
    "Here we add `df_matches_html` to `df_matches`. This contains each players inducements as a single string, not convenient for analysis.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches = pd.merge(df_matches, df_matches_html, on='match_id', how='left')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add specific inducement info to df_matches\n",
    "\n",
    "The `inducements` dataframe cannot easily be added to `df_matches`. We can however, extract information from `inducements` at the match level and add this to `df_matches`. Here, I show how to add a 1/0 flag `has_sp` that codes for if a match included any star player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sp = (inducements\n",
    "            .groupby(\"match_id\")\n",
    "            .agg(has_sp = (\"star_player\", \"max\"))\n",
    "            .reset_index()\n",
    ")\n",
    "\n",
    "\n",
    "df_matches = pd.merge(df_matches, df_sp, on = \"match_id\", how = \"left\")\n",
    "\n",
    "df_matches['match_id'] = pd.to_numeric(df_matches.match_id) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# steps 4-6 add metadata to matches\n",
    "\n",
    "The match JSON is the source for division and division name, these are not present in the tournament endpoint. This is a team property.\n",
    "The tournament JSON Is the source for the group_id (league), these are not present in the match endpoint.\n",
    "The group JSON is the source for the group name and ruleset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "question: is it possible to add league (number / id), and possibly ruleset,  to the match endpoint?\n",
    "Not really, because I don't have it stored there\n",
    "The thing is that teams can be moved across leagues (and therefore rulesets)\n",
    "So you can't know for sure which was being used at the time of the game\n",
    "So even if I take league id from the teams endpoint, it might not be correct for all matches played by that team? Correct.\n",
    "And even for a given league, the base ruleset can change over time (which won't be tracked)\n",
    "And for a given ruleset, there's no guarantee that the settings are the same over time.\n",
    "Much in the same way you can't really look at the current state of a team to see which players were in a game for a given match in the past.\n",
    "You could update nightly for matches really if you wanted to\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 add tournament info\n",
    "\n",
    "create list of all tournament ids.\n",
    "Fetch and store as JSON.\n",
    "Then process and add tournament names to `df_matches`.\n",
    "\n",
    "There are some 4K tournaments in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make list of all tournaments that need to be fetched\n",
    "\n",
    "tournament_ids = df_matches['tournament_id'].values\n",
    "\n",
    "# get unique values by converting to a Python set and back to list\n",
    "tournament_ids = list(set(tournament_ids))\n",
    "\n",
    "len(tournament_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_tourney_jsons(tournament_ids, fullrun = FETCH_NEW_TOURNEYS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process the tourney json files from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tourneys = process_tourney_jsons(tournament_ids, fullrun = PROCESS_TOURNEYS, target = DF_TOURNEYS_RAW)\n",
    "\n",
    "df_tourneys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tourneys.sort_values(by='tournament_id', inplace=True, ascending=False)\n",
    "df_tourneys = df_tourneys.reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 Fetch groups (for groupname and ruleset)\n",
    "\n",
    "From Discord somewhere:\n",
    "```\n",
    "Ok, let me start from the beginning with terminology ðŸ™‚\n",
    "A ruleset is basically the root of the whole thing at this point. This is where rosters and rule options are defined.\n",
    "A division is an environment for \"open play\", and used to be the root of the data model before rulesets were a thing.\n",
    "A league is a special kind of group, which is defined by it having a ruleset configured.\n",
    "A group is a \"meta group\" and currently acts mostly as a parent for tournaments\n",
    "A tournament is the actual scheduling systems put into play.\n",
    "```\n",
    "A \"League\" is a special kind of \"Group\"?\n",
    "\n",
    "Yes, they are technically the same, but the league is configured with a custom ruleset (and therefore doesn't use the division default ruleset)\n",
    "The distinction between a group and a league is very narrow, but has a huge effect.\n",
    "In a sense the \"league\" is equivalent to a custom division.\n",
    "https://discord.com/channels/254387387260469258/739746315449139240/1080863062342504478"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_ids = df_tourneys['group_id'].values\n",
    "\n",
    "# get unique values by converting to a Python set and back to list\n",
    "group_ids = list(set(group_ids))\n",
    "\n",
    "len(group_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_group_jsons(group_ids, fullrun = FETCH_NEW_GROUPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groups = process_group_jsons(group_ids, fullrun = PROCESS_GROUPS, target = DF_GROUPS_RAW)\n",
    "df_groups = df_groups.rename(columns={\"ruleset\": \"current_ruleset\"})\n",
    "df_groups.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tourneys2 = pd.merge(df_tourneys, df_groups, on = 'group_id', how = 'left') # lost a few\n",
    "\n",
    "df_tourneys2.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add tourney, group and division data to match data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches.iloc[[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches = pd.merge(df_matches, df_tourneys2, on='tournament_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches.query(\"match_id == 4421729\")\n",
    "df_matches.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6  Add ruleset_version and division_name to df_matches\n",
    "\n",
    "Divisions are very important in FUMBBL, every team belongs to a Division.\n",
    "\n",
    "PM we see that (NAF) matches played previously under ruleset 2228 are now labeled as ruleset 2310? this has a few changes (tier, gold, crossleague)\n",
    "* Do we also see this in the XML API\n",
    "\n",
    "Lets have look at the various divisions and leagues, and which rulesets are used.\n",
    "There are a lot of small leagues being played on FUMBBL, they account for maybe X% of all the matches.\n",
    "\n",
    "We only look at divisions and leagues with a sufficient volume of matches, or otherwise we do not have sufficient statistics for each race.\n",
    "\n",
    "So I aggregated the data by division, league and ruleset, and filtered on at least 150 different teams that have played at least once last year.\n",
    "Apart from the main \"Divisions\" that are part of FUMBBL, there were a few user-run leagues present in this table, so I looked up their names on FUMBBL and what ruleset is used (BB2016, BB2020 or some other variant). This information (contained in an xlsx) is added to the dataset below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = (df_matches\n",
    ".groupby([ 'division_id', 'division_name', 'scheduler' , 'year', 'group_id', 'group_name',  'tournament_id' , 'tournament_name'], dropna=False)\n",
    ".agg(\n",
    "    n_games = ('match_id', 'count')\n",
    ")\n",
    ".reset_index()\n",
    ".sort_values(\"n_games\", ascending=False)\n",
    ")\n",
    "\n",
    "#res.to_excel('group_counts.xlsx')\n",
    "res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conclusion, major divisions are not found. Those (teams /) matches do not have a tournament id.\n",
    "But clearly distinguished.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7 fetch Blackbox Trophy metadata\n",
    "\n",
    "Fetch for each BBT season the team id list of participating teams, and number of BBT games played. \n",
    "Use this to label BBT games in the match dataset.\n",
    "\n",
    "PM needs updated Dev environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/scrape_match_htmls.py\n",
    "\n",
    "#scrape_single_html()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save all prepped datasets as CSV files\n",
    "\n",
    "\n",
    "CSV files are the lingua franca across all data analysis software.\n",
    "\n",
    "A dataset release consists of three datasets:\n",
    "* A list of matches, identified by match_id\n",
    "* A list of matches by team, identified by match_id and team_id\n",
    "* A list of inducements by match_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to locate variables that cannot be serialized by hdf5\n",
    "#df_matches.loc[:, :'week_date'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'datasets/current/df_matches'\n",
    "\n",
    "df_matches.to_csv(target + '.csv')\n",
    "\n",
    "#df_matches.to_parquet(target + '.parquet') # this causes a kernel crash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'datasets/current/inducements'\n",
    "#inducements.to_hdf(target + '.h5', key='inducements', mode='w', format = 't',  complevel = 9)\n",
    "inducements.to_csv(target + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'datasets/current/df_tourneys'\n",
    "\n",
    "#df_tourneys.to_hdf(target + '.h5', key='df_tourneys', mode='w', format = 't',  complevel = 9)\n",
    "df_tourneys.to_csv(target + '.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing a license for the public dataset\n",
    "\n",
    "An important part of making data publicly available is being explicit about what is allowed if people want to use the dataset.\n",
    "However, before we do so, we have to check if **we** are actually allowed to publish the data. This is explained nicely [in a blogpost by Elizabeth Wickes](https://datacarpentry.org/blog/2016/06/data-licensing).\n",
    "\n",
    "Since our data comes from the **FUMBBL.com** website, we check the [**Privacy policy**](https://fumbbl.com/p/privacy) where all users, including myself have agreed on when signing up. It contains this part which is specific to the unauthenticated API, which we use to fetch the data, as well as additional public match data, such as which inducements are used in a match, and the Coach rankings of the playing coaches that were current when the match was played.\n",
    "\n",
    "```\n",
    "Content you provide through the website\n",
    "All the information you provide through the website is processed by FUMBBL. This includes things such as forum posts, private message posts, blog entries, team and player names and biographies and news comments. Data provided this way is visible by other people on the website and in most cases public even to individuals without accounts (not including private messages), and as such are considered of public interest. If direct personal information is posted in public view, you can contact moderators to resolve this. Match records are also considered content in this context, and is also considered of public interest. This data is collected as the primary purpose of the website and it is of course entirely up to you how much of this is provided to FUMBBL. \n",
    "\n",
    "Third party sharing\n",
    "Some of the public data is available through a public (*i.e. unauthenticated*) API, which shares some of the information provided by FUMBBL users in a way suitable for third-party websites and services to process.\n",
    "\n",
    "The data available through the unauthenticated API is considered non-personal as it only reflects information that is public by its nature on the website. The authenticated API will only show information connected to the authenticated account.\n",
    "```\n",
    "\n",
    "I conclude that since the match data is already considered public content, there is no harm in collecting this public data in a structured dataset and placing this data in a public repository. I also verified this with Christer, the site owner. \n",
    "\n",
    "\n",
    "The final step is then to decide what others are allowed to do with this data. In practice, this means choosing a license under which to release the dataset. I decided to choose a CC0 license: this places the data in the public domain, and people can use the dataset as they wish. Citing or mentioning the source of the data would still be appreciated of course."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of possible future improvements\n",
    "\n",
    "* Scraping the players (only most recent version, so no player development history)\n",
    "* Scraping the rulesets (for example to identify resurrection tournaments where players choose skills and use tiers)\n",
    "\n",
    "* catch exception: **PM we cannot deal yet with the situation HTTPSConnectionPool(host='fumbbl.com', port=443): Max retries exceeded with url: /api/match/get/4221820 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f4acff12be0>: Failed to establish a new connection: [Errno 110] Connection timed out',))**\n",
    "\n",
    "* cr_bin variable is gone?\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "50276fd1884268afe39607052f22ef19b84d915691d702a5c7e9a67a09867105"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('requests_env': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
